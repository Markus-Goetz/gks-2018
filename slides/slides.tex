\documentclass[aspectratio=169]{beamer}

\usetheme{HGF}

\usepackage{amsmath}
\usepackage{caption}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{xmpmulti}
\usepackage{xpatch}

\xpatchcmd{\itemize}
{\def\makelabel}
{\ifnum\@itemdepth=1\relax
    \setlength\itemsep{2ex}% separation for first level
    \else
    \ifnum\@itemdepth=2\relax
    \setlength\itemsep{1ex}% separation for second level
    \else
    \ifnum\@itemdepth=3\relax
    \setlength\itemsep{0.5ex}% separation for third level
    \fi\fi\fi\def\makelabel
}
{}
{}

\pgfplotsset{compat=newest}

\graphicspath{{./images/}}
\makeatletter
\def\input@path{{./}{./images}}
\makeatother

\newcommand\imageright[1]{ %
    \caption*{\scalebox{.5}{\textcolor{lightgray}{\textcopyright~#1}}} %
}

\DeclareMathOperator*{\argmin}{arg\,min}

\title{Machine Learning with Neural Networks}
\subtitle{GridKa School 2018}
\author{Markus GÃ¶tz}
\date{2018-08-29}
\institute{KIT}

\begin{document}
    
\section{Introduction}
\label{sec:introduction}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Outline}
    \tableofcontents[hideallsubsections]
\end{frame}

\section{Machine Learning Fundamentals}
\label{sec:machine-learning}

\subsection{Terminology}
\label{subsec:terminology}

\begin{frame}
\frametitle{Terminology}
\begin{columns}
    \begin{column}{0.48\textwidth}
        \begin{itemize}
            \item \textbf{Samples} or instances,\\ 
            individual observations in your data,\\
            e.g. an image, a specimen
            \item \textbf{Features} or attributes,\\ 
            single characteristic of a sample,\\
             e.g. a pixel, measured weight
            \item \textbf{Channels} or time,\\
            depth information,\\
            color channels, change over time
        \end{itemize}
    \end{column}
    \begin{column}{0.48\textwidth}
        \begin{figure}
            \includegraphics{terminology.png}
        \end{figure}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}
    \frametitle{Terminology}
    
    \begin{figure}
        \centering
        \begin{tikzpicture}[fill=white, font=\footnotesize]
            \draw (0, 0) ellipse (7.0 and 3.5);
            \node[align=left] at (5.1, 0.0) {\textbf{Artificial}\\ \textbf{Intelligence (AI)}\\e.g. rule-based system};
            
            \draw (-1.7, 0) ellipse (4.8 and 2.7) (1.8, 0.0) node[align=left] {\textbf{Machine}\\ \textbf{Learning}\\ e.g. Logistic\\ Regression};
            
            \draw (-2.7, 0) ellipse (3.3 and 1.9) (-2.7, 0.0) node[align=left] {\textbf{Deep Learning}\\ e.g. Convolutional\\ Neural Networks};
        \end{tikzpicture}
    \end{figure}
\end{frame}

\begin{frame}
\frametitle{Why now?}
    \begin{itemize}
        \item \textbf{Technology revolution}---vector processors (e.g. GPGPUs), auto-gradient software
        \item \textbf{Data availability}---large, partially freely available, collections of labeled data
        \item \textbf{Mathematical advances}---latest addition, investigation of new model elements, e.g. activation functions, normalization
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Learning Approaches}
    \begin{itemize}
        \item \textbf{Supervised learning:} Learn by ``mimicking supervisor'', i.e. pattern annotations\\ 
        examples: image classification, stock forecasting
        \item \textbf{Unsupervised learning:} Determine patterns purely based on data\\ examples: customer cluster analysis, distribution estimation
        \item \textbf{Reinforcement learning:} Pavlov-style learning with punishment and reward in dynamic environments\\
        examples: game AIs, e.g. AlphaGo or Dota OpenAI
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Notation Disclaimer}
\begin{itemize}
    \item \textbf{Small letters:} vectors or matrices, e.g. $x$ or $y$
    \item \textbf{Hats:} predictions or estimates, e.g. $\hat{y}$
    \item \textbf{Indices:} elements of vectors and matrices, e.g. $x_{i}$
\end{itemize}

\medskip
\end{frame}

\begin{frame}
\frametitle{Linear Regression}
    \begin{columns}
        \begin{column}{0.43\textwidth}
            \begin{figure}
                \begin{tikzpicture}
                    \begin{axis}[
                        width=\linewidth, 
                        grid=both,
                        grid style={line width=.3pt, draw=blue!10},
                        major grid style={line width=.3pt, draw=blue!10},
                        minor tick num=3,
                        xlabel={$x$},
                        ylabel={$y$}, 
                        ticks=none,
                        axis x line=center,
                        axis y line=center,
                        xmin=-0.1,
                        xmax=3.0,
                        ymin=-0.1,
                        ymax=1.6
                    ]
                    % domain=0.25:2.75
                    \addplot [red, thick, mark=none] {0.5*x};
                    \node at (axis cs:2.0,1.3) [red] {\footnotesize\emph{model}};
                    \addplot [blue, only marks, mark size=1pt] coordinates {
                        (0.3, 0.10054215032765762)
                        (0.4263157894736842, 0.12363160260273953)
                        (0.5526315789473685, 0.2228985962683488)
                        (0.6789473684210527, 0.32110283680834006)
                        (0.8052631578947369, 0.3464254484709207)
                        (0.9315789473684213, 0.530973472751518)
                        (1.0578947368421054, 0.508700039361571)
                        (1.1842105263157896, 0.5115642913251457)
                        (1.310526315789474, 0.6517708510332498)
                        (1.4368421052631581, 0.7372626388813315)
                        (1.5631578947368425, 0.7555802030144961)
                        (1.6894736842105267, 0.8020873099226213)
                        (1.8157894736842108, 0.920859494738636)
                        (1.9421052631578952, 0.9025123937365214)
                        (2.0684210526315794, 1.0124087289206862)
                        (2.1947368421052635, 1.081906810760111)
                        (2.3210526315789477, 1.089021024213448)
                        (2.447368421052632, 1.3056222105270976)
                        (2.573684210526316, 1.2573605236570795)
                        (2.7, 1.2546989788028189)
                    };
                    \node at (axis cs:1.8,0.5) [blue] {\footnotesize\emph{data}};
                    \end{axis} 
                \end{tikzpicture}
            \end{figure}
        \end{column}
        \begin{column}{0.53\textwidth}
            \begin{itemize}
                \item<1-> \textbf{Data set} $\{samples, labels\}=\{x, y\}$
                \item<2-> \textbf{Model} definition $\hat{y}=wx+b$\\ with $w$ and $b$ trainable parameters
                \item<3-> \textbf{Loss function} or cost/objective\\
                $J(w,b)=MSE(w,b)=\frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y_i})^2$
                \item<4-> \textbf{Train} the model, e.g. optimization\\
                $\hat{w}, \hat{b}=\argmin J(w, b)$ \\
                
            \end{itemize}
        \end{column}
    \end{columns}

    \bigskip
    \begin{itemize}
        \item<5>
            \begin{center}
                \textbf{Basic recipe for most machine learning algorithms}
            \end{center}
    \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Optimization: Gradient Descent}

\begin{itemize}
    \item Iterative optimization technique, weight update in direction of negative gradient
\end{itemize}
\begin{center}$w_{i+1}=w_{i}-lr\nabla_{w_{i}}J(w)$\end{center}
\vspace{-0.7cm}
\begin{columns}
    \begin{column}{0.48\textwidth}
        \begin{figure}
            \begin{tikzpicture}
            \begin{axis}[
                width=\linewidth,
                height=0.6\textheight, 
                grid=both,
                grid style={line width=.3pt, draw=blue!10},
                major grid style={line width=.3pt, draw=blue!10},
                minor tick num=3,
                xlabel={$x$},
                ylabel={$y$}, 
                ticks=none,
                axis x line=center,
                axis y line=center,
                xmin=-0.1,
                xmax=3.0,
                ymin=-0.1,
                ymax=1.6
            ]
            
            \addplot [red, thick, mark=none, dashed, domain=0:4] {0.1*(x - 1.5) + 0.75};
            \node at (axis cs:0.3,0.7) [red] {\footnotesize$w_{1}$};
            \addplot [red, thick, mark=none, dashed, domain=0:4] {0.25*(x - 1.5) + 0.75)};
            \node at (axis cs:0.3,0.35) [red] {\footnotesize$w_{2}$};
            \addplot [red, thick, mark=none] {0.5*x};
            \node at (axis cs:2.2,1.3) [red] {\footnotesize$w_{s}$};
            
            
            \addplot [blue, only marks, mark size=1pt] coordinates {
                (0.3, 0.10054215032765762)
                (0.4263157894736842, 0.12363160260273953)
                (0.5526315789473685, 0.2228985962683488)
                (0.6789473684210527, 0.32110283680834006)
                (0.8052631578947369, 0.3464254484709207)
                (0.9315789473684213, 0.530973472751518)
                (1.0578947368421054, 0.508700039361571)
                (1.1842105263157896, 0.5115642913251457)
                (1.310526315789474, 0.6517708510332498)
                (1.4368421052631581, 0.7372626388813315)
                (1.5631578947368425, 0.7555802030144961)
                (1.6894736842105267, 0.8020873099226213)
                (1.8157894736842108, 0.920859494738636)
                (1.9421052631578952, 0.9025123937365214)
                (2.0684210526315794, 1.0124087289206862)
                (2.1947368421052635, 1.081906810760111)
                (2.3210526315789477, 1.089021024213448)
                (2.447368421052632, 1.3056222105270976)
                (2.573684210526316, 1.2573605236570795)
                (2.7, 1.2546989788028189)
            };
            \node at (axis cs:1.8,0.5) [blue] {\footnotesize\emph{data}};
            \end{axis} 
            \end{tikzpicture}
        \end{figure}
    \end{column}
    \begin{column}{0.48\textwidth}
        \begin{figure}
            \begin{tikzpicture}
            \begin{axis}[
            width=\linewidth, 
            height=0.6\textheight, 
            grid=both,
            grid style={line width=.3pt, draw=blue!10},
            major grid style={line width=.3pt, draw=blue!10},
            minor tick num=3,
            xlabel={$w$},
            ylabel={$J$}, 
            ticks=none,
            axis x line=center,
            axis y line=center,
            xmin=-0.1,
            xmax=3.0,
            ymin=-0.1,
            ymax=1.6
            ]
            % domain=0.25:2.75
            \addplot [black, thick, mark=none, smooth, samples=50, domain=0.3:2.7] {0.9*(x-1.5)^2 + 0.2};
            \addplot [red, only marks, mark size=2pt] coordinates {
                (1.5, 0.2)
                (1.0, 0.425)
                (0.5, 1.1)
            };
            \addplot [red, mark=none] coordinates {
                (1.5, 0.05)
                (1.5, 0.15)
            };
            \addplot [red, mark=none] coordinates {
                (1.0, 0.05)
                (1.0, 0.37)
            };
            \addplot [red, mark=none] coordinates {
                (0.5, 0.05)
                (0.5, 1.00)
            };
            \addplot [blue, mark=none] coordinates {
                (0.53, 0.95)
                (0.53, 0.43)
                (0.93, 0.43)
            };
            \draw [->, red](axis cs:0.57,1.07)--(axis cs:0.98,0.48);
            \node at (axis cs:1.20,0.8) [blue] {$lr\frac{dJ}{dw}$};
            \node at (axis cs:0.7,0.1) [red] {\footnotesize$w_{1}$};
            \node at (axis cs:1.2,0.1) [red] {\footnotesize$w_{2}$};
            \node at (axis cs:1.7,0.1) [red] {\footnotesize$w_{s}$};
            \end{axis} 
            \end{tikzpicture}
        \end{figure}
    \end{column}
\end{columns}
\begin{itemize}
    \item $lr$ is learning rate, gradient update factor
    \item \textbf{Stochastic gradient descent (SGD)}, sample subset (\textbf{batch}) updates
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Bias Trick}

\begin{itemize}
\item Cumbersome to keep track of weights $w$ and bias $b$
\item \textbf{Idea:} fuse both into single weight matrix

\begin{alignat*}{11}
\hat{y}&=&w&x&+b&\leftrightarrow&\hat{y}&=&w&x \\
\hat{y}&=&\begin{pmatrix}
w_{1} \\
w_{2} \\
\vdots \\
w_{n}
\end{pmatrix}'&
\begin{pmatrix}
x_{i,1}\\
x_{i,2}\\
\vdots\\
x_{i,n}\\
\end{pmatrix}&+b&\leftrightarrow&\hat{y}&=&
\begin{pmatrix}
\textbf{b} \\
w_{1} \\
w_{2} \\
\vdots \\
w_{n}
\end{pmatrix}'&
\begin{pmatrix}
\textbf{1}\\
x_{i,1}\\
x_{i,2}\\
\vdots\\
x_{i,n}\\
\end{pmatrix}
\end{alignat*}
\end{itemize}
\end{frame}

\subsection{Frameworks}
\label{subsec:frameworks}

\subsection{Exercise: MNIST Dataset}
\label{subsec:fnn-exercise}

\begin{frame}
    \frametitle{Exercise 1: Warm-up}
    \begin{figure}
        \centering
        \begin{tikzpicture}[]
            \node [inner sep=0pt,above right] { %
                \includegraphics[width=0.5\linewidth]{jupyter.png} %
            };
            % show origin
            \draw [line width=0.6mm, red] (7.05, 4.05) ellipse (0.5 and 0.25);
        \end{tikzpicture}
    \end{figure}
\end{frame}

\subsection{Logistic Regression}
\label{subsec:logistic-regression}

\begin{frame}
\frametitle{Pattern Recognition Types}
\begin{itemize}
    \item \textbf{Regression:} predict continuous value, e.g. stock price, $y\in\mathbb{R}^{c}$
    \item \textbf{Classification:}
\end{itemize}
\end{frame}

\subsection{Exercise: Logistic Regression}
\label{subsec:exercise-logistic-regression}

\begin{frame}
    \frametitle{Exercise: Logistic Regression}
    \begin{figure}
        \centering
    \end{figure}
\end{frame}

\section{Neural Networks}
\label{sec:neural-networks}

\begin{frame}
\frametitle{XOR-Problem}
\begin{columns}
    \begin{column}{0.48\textwidth}
        \begin{figure}
            \begin{tikzpicture}
            \begin{axis}[
                width=0.9\linewidth,
                grid=major,
                grid style={line width=.3pt, draw=blue!10},
                major grid style={line width=.3pt, draw=blue!10},
                %minor tick num=2,
                xlabel={$x_1$},
                ylabel={$x_2$},
                xmin=-0.1,
                xmax=1.1,
                ymin=-0.1,
                ymax=1.1
            ]
            \node[draw, circle, fill=red,  radius=0.3cm] at (1, 1) {};
            \node[draw, circle, fill=red,  radius=0.3cm] at (0, 0) {};
            \node[draw, circle, fill=blue, radius=0.3cm] at (1, 0) {};
            \node[draw, circle, fill=blue, radius=0.3cm] at (0, 1) {};
            \end{axis} 
            \end{tikzpicture}
        \end{figure}
    \end{column}
    \begin{column}{0.48\textwidth}
        \begin{itemize}
            \item 
        \end{itemize}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Fully-connected Neural Network}

\begin{columns}
    \begin{column}{0.48\textwidth}
        \begin{figure}
            \begin{tikzpicture}
            \node[draw, circle, thick, minimum size=0.8cm] (input1) at (-2,-0.7) {\footnotesize$x_1$};
            \node[draw, circle, thick, minimum size=0.8cm] (input2) at (-2, 0.7) {\footnotesize$x_2$};
            
            \node[draw, circle, thick, minimum size=0.8cm] (hidden1) at (0,-1.4) {\footnotesize$h_1$};
            \node[draw, circle, thick, minimum size=0.8cm] (hidden2) at (0, 0)   {\footnotesize$h_2$};
            \node[draw, circle, thick, minimum size=0.8cm] (hidden3) at (0, 1.4) {\footnotesize$h_3$};
            
            \node[draw, circle, thick, minimum size=0.8cm] (output) at (2, 0) {\footnotesize$\hat{y}$};
            
            \draw[->, >=stealth, thick] (input1)--(hidden1);
            \draw[->, >=stealth, thick] (input1)--(hidden2);
            \draw[->, >=stealth, thick] (input1)--(hidden3);
            
            \draw[->, >=stealth, thick] (input2)--(hidden1);
            \draw[->, >=stealth, thick] (input2)--(hidden2);
            \draw[->, >=stealth, thick] (input2)--(hidden3);
            
            \draw[->, >=stealth, thick] (hidden1)--(output);
            \draw[->, >=stealth, thick] (hidden2)--(output);
            \draw[->, >=stealth, thick] (hidden3)--(output);
            
            \node at (-2, -2.5) {\footnotesize \textbf{Input}};
            \node at ( 0, -2.5) {\footnotesize \textbf{Hidden}};
            \node at ( 2, -2.5) {\footnotesize \textbf{Output}};
            \end{tikzpicture}
        \end{figure}
    \end{column}
    \begin{column}{0.48\textwidth}
    \end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Universal Approximation Theorem}

\uncover<1->{
A feed-forward neural network with a linear output and at least one hidden layer can approximate any reasonable function to arbitrary precision with a finite number of nodes.
}
\vspace{0.5cm}
\begin{itemize}
    \item<2-> \textbf{Good News}
    \begin{itemize}
        \item Networks can perform highly complex tasks
        \item All necessary ingredients available
    \end{itemize}
    \item<3-> \textbf{Bad News}
    \begin{itemize}
        \item Does not specify number of necessary nodes
        \item No remarks on neuron connectivity
        \item In practice: deep networks perform better
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Multi-class Classification}

\begin{itemize}
    \item \textbf{One-hot class encoding:} encode classes as sparse vectors \\
    $y=(y_1, y_2, ..., y_c)$, only one is active, e.g. class $2\rightarrow (0, 1, ..., 0)$
    \item \textbf{Softmax output activation:} $\hat{y}=softmax(z)=\frac{e^{z_{j}}}{\sum_{j}e^{z_{j}}}$ for $j=1...c$\\
    achieve joint-probability of $1$, normalize across model outputs $z$
    \item \textbf{Cross-entropy loss:} convex-function $J(w)=\frac{1}{n}\sum_{i=1}^{n}\sum_{j}^{c}y_{i,j}\log\hat{y_{i,j}}$\\
    maximum likelihood principle
\end{itemize}
\end{frame}

\subsection{Additional Components}
\label{subsec:additional-components}

\begin{frame}
\frametitle{Activation Functions}

\begin{itemize}
    \item Activation functions $a(x)$ introduce \textbf{non-linearity}, e.g. sigmoid function
    \item Other non-linear choices, e.g. $tanh(x)$, $relu(x)=max(0,x)$, etc.
\end{itemize}

\begin{figure}
    \begin{tikzpicture}
    \begin{axis}[
        title=sigmoid,
        width=0.3\linewidth, 
        grid=major,
        grid style={dashed, line width=.3pt, draw=blue!10},
        major grid style={line width=.3pt, draw=blue!10},
        xlabel={$x$},
        ylabel={$a(x)$},
        xmin=-2,
        xmax=2,
        ymin=-2,
        ymax=2
    ]
    \addplot [blue, thick, mark=none, smooth, samples=50] {1/(1+e^(-x))};
    \end{axis} 
    \end{tikzpicture}\begin{tikzpicture}
    \begin{axis}[
        title=tanh,
        width=0.3\linewidth, 
        grid=major,
        grid style={dashed, line width=.3pt, draw=blue!10},
        major grid style={line width=.3pt, draw=blue!10},
        yticklabels={,,},
        xlabel={$x$},
        xmin=-2,
        xmax=2,
        ymin=-2,
        ymax=2
    ]
    \addplot [blue, thick, mark=none, smooth, samples=50] {tanh(x)};
    \end{axis}
    \end{tikzpicture}\begin{tikzpicture}
    \begin{axis}[
        title=ReLU,
        width=0.3\linewidth, 
        grid=major,
        grid style={dashed, line width=.3pt, draw=blue!10},
        major grid style={line width=.3pt, draw=blue!10},
        yticklabels={,,},
        xlabel={$x$},
        xmin=-2,
        xmax=2,
        ymin=-2,
        ymax=2
    ]
    \addplot [blue, thick, mark=none, domain=-3:3] {max(0, x)};
    \end{axis}
    \end{tikzpicture}\begin{tikzpicture}
    \begin{axis}[
        title=SeLU,
        width=0.3\linewidth, 
        grid=major,
        grid style={dashed, line width=.3pt, draw=blue!10},
        major grid style={line width=.3pt, draw=blue!10},
        yticklabels={,,},
        xlabel={$x$},
        xmin=-2,
        xmax=2,
        ymin=-2,
        ymax=2
    ]
    \addplot [blue, thick, mark=none, smooth, samples=50, domain=-3:0]{1.0507 * 1.6732 * (e^x - 1)};
    \addplot [blue, thick, mark=none, smooth, samples=50, domain=0:3]{x};
    \end{axis}
    \end{tikzpicture}
\end{figure}
\end{frame}

\subsection{Backpropagation}
\label{subsec:Backpropagation}

\subsection{Generalization}
\label{subsec:generalization}

\subsection{Exercise: MNIST FNN}
\label{subsec:exercise-fnn}

\begin{frame}
    \frametitle{Exercise: FNN MNIST Image Classification}
    
    \begin{figure}
        \centering
        \includegraphics[width=0.4\linewidth]{mnist.png}
    \end{figure}
\end{frame}

\section{Convolutional Neural Networks}
\label{sec:convolutional-neural-networks}

\subsection{Discrete Convolution}
\label{subsec:discrete-convolution}

\begin{frame}
    \frametitle{Discrete Convolution}
    \begin{figure}
        \centering
        \multiinclude[<+->][format=png, graphics={width=0.8\linewidth}]{convolution}
        \imageright{Machine Learning Guru}
    \end{figure}
\end{frame}

\subsection{Network Architectures}
\label{subsec:network-architectures}

\subsection{Exercise: MNIST CNN}
\label{subsec:cnn-exercise}

\begin{frame}
\frametitle{Exercise: FNN MNIST Image Classification}

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{mnist.png}
\end{figure}
\end{frame}

\section{Regression}
\label{sec:regression}

\subsection{Exercise: Abalone}
\label{subsec:regression-exercise}

\begin{frame}
    \frametitle{Exercise: Abalone Age Regression Analysis}
    \begin{figure}
        \centering
        \includegraphics[width=0.7\linewidth]{abalone.jpg}
        \imageright{Garnelaxia}
    \end{figure}
\end{frame}

\section{Summary}
\label{sec:summary}

\begin{frame}
\frametitle{Summary}

\begin{itemize}
    \item Supervised machine learning
    \begin{itemize}
        \item Logistic regression
        \item (Convolutional) neural networks
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Open Topics}
\end{frame}

\begin{frame}
\frametitle{Acknowledgment}

\begin{columns}
    \begin{column}{0.48\textwidth}
        \begin{itemize}
            \item \textbf{Eileen KÃ¼hn}
            \begin{itemize}
                \item GridKa School organization
                \item Paperwork
            \end{itemize}
            \item \textbf{Oskar Taubert}
            \begin{itemize}
                \item Assignment preparation
                \item Exercise supervision
            \end{itemize}
            \item \textbf{Andreas Herten}
            \begin{itemize}
                \item Access to JURON
                \item Technical support
            \end{itemize}
        \end{itemize}
    \end{column}

    \begin{column}{0.48\textwidth}
        \begin{figure}
            \includegraphics[width=0.25\linewidth]{eileen.jpg}\quad \includegraphics[width=0.25\linewidth]{oskar.jpg} \\\medskip
            
            \includegraphics[width=0.25\linewidth]{andreas.jpg}\quad \includegraphics[width=0.25\linewidth]{juron.jpg} \\
        \end{figure}
    \end{column}
\end{columns}
\end{frame}

\end{document}
