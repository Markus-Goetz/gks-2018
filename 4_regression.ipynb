{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis with Keras\n",
    "\n",
    "Neural networks are not only able to perform classification task as previously introduced, but can also forecast continuous values. This type of prediction task is often called regression or regression analysis. In principle, the neural networks' architectures are constructed analogous to the previous examples, except for a few changes. First, the activation function for the output neuron needs to be changed. A value between zero and one, as the sigmoid function would output is meaningless. Therefore, it is changed to be the linear identity function $f(x)=x$. Second, an equally important, is the consideration of the loss function to be optimized. Instead of measuring a binary choice, the distance to the continuous variable needs to be expressed. There is a wide variety of possible loss functions. In practice, however, the mean-square-error (mse) $\\overline{(y - \\hat{y})^2}$ or the mean-absolute-deviation (mae) $|y - \\hat{y}|$ is frequently used.\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "For a detailed explanation of the used modules, please refer to the respective sections in the [introductory notebook](0_MNIST_dataset.ipynb) and [logistic regression notebook](1_logistic_regression.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import Sequence\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "This time, we are going to work on a different dataset called 'abalone'. It stems from the field of biology and records physical properties of 4177 abalones (\"sea snails\"), like their height, width, weight etc. Again, the data is stored in an HDF5 file with two datasets, one containing the actual data, and another with the names of the measured properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path='abalone.h5'):\n",
    "    \"\"\"\n",
    "    Loads a dataset and its column names from the HDF5 file specified by the path.\n",
    "    It is assumed that the HDF5 dataset containing the data is called 'data' and the columns are called 'columns'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str, optional\n",
    "        The absolute or relative path to the HDF5 file, defaults to mnist.h5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_and_labels : tuple(np.array[samples, features], np.array[features])\n",
    "        a tuple with two numpy array containing the data and column names\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as handle:\n",
    "        return np.array(handle['data']), list(column.decode('utf-8') for column in handle['columns'])\n",
    "    \n",
    "data, columns = load_data()\n",
    "data.shape, columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data at a Glance\n",
    "\n",
    "Let's have a very brief look at the data by plotting each of the nine features in a histogram. You will note, one curiosity about the actually categorical 'sex' column. The creators of the dataset have encoded three possible value, -1 for male, 1 for female and additionally 0 for infants. All the other observables do not expose any particularities.\n",
    "\n",
    "The prediction task at hand is to forecast how old and abalone is-their ring count + 1,5 years. It is possible to determine the ring count using a combination of colorants and microscopic work. This is somewhat a time consuming and expensive procedure, we would like to replace by a solid prediction based on proxy value, i.e. physical properties of the abalone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bins(data, columns):\n",
    "    \"\"\"\n",
    "    Plots the histograms of each of the passed columns of the data set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.array([samples, features])\n",
    "        The data to be plotted.\n",
    "    path : list\n",
    "        The corresponding column names\n",
    "    \"\"\"\n",
    "    features = data.shape[1]\n",
    "    figure, axis = plt.subplots(3, features // 3, figsize=(16, 8))\n",
    "    axis = np.array(axis).flatten()\n",
    "    \n",
    "    for i, variable in enumerate(columns):\n",
    "        axis[i].hist(data[:, i], bins=30)\n",
    "        axis[i].set_xlabel(variable)\n",
    "        axis[i].set_ylabel('count')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_bins(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The abalone dataset is already in good shape when it comes to data cleaning, i.e. no missing values, removed noise, etc., but need to be further preprocessed before we are able to feed it to a neural network. While somewhat close, the value ranges of the input variables are spread differently. We need to bring to normalize these to a common value system. Here, the mean of each individual feature will be substracted and then scaled by its standard deviation. This way, all variable are expressed as multiple of their own variation. It is important to determine these values from the training data partition alone. Otherwise, information from the test data would leak into the training process. Additionally, we are going to split off the feature to be predicted (rings) and divide the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_preprocess(data, train_fraction=3.0/4.0):    \n",
    "    \"\"\"\n",
    "    Preprocesses that dataset by normalizing to mean-standard deviation and partitioning the data \n",
    "    into training and test data and corresponding labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.array([samples, features])\n",
    "        The data to be preprocessed and normalized\n",
    "    train_fraction : float\n",
    "        Fraction of samples to be assigned to the training dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_data, train_labels, test_data, test_labels : \n",
    "    tuple(np.array[train samples, features], np.array[train samples],\n",
    "          np.array[test samples, features], np.array[test samples]\n",
    "    )\n",
    "        a tuple with four numpy array containing the training and test data and labels\n",
    "    \"\"\"\n",
    "    split_point = int(data.shape[0] * train_fraction)\n",
    "    \n",
    "    # split the data in train, test and corresponding labels\n",
    "    label_index = -1\n",
    "    train_labels, test_labels = data[:split_point, label_index], data[split_point:, label_index]\n",
    "    train, test = data[:split_point, :label_index], data[split_point:, :label_index]\n",
    "    \n",
    "    # calculate the mean and standard deviation for each feature for normalization\n",
    "    mean, sigma = train.mean(axis=0), train.std(axis=0)\n",
    "    # do not normalize the categorical 'sex' column\n",
    "    mean[0], sigma[0] = 0.0, 1.0\n",
    "    \n",
    "    return (train - mean) / sigma, train_labels, (test - mean) / sigma, test_labels\n",
    "    \n",
    "train_data, train_labels, test_data, test_labels = split_and_preprocess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Appropriate Network\n",
    "\n",
    "As described in the introduction, we will be using one of the previously neural network architectures-the fully-connected network. The most notable changes are the activation function of the output neuron, the MAE loss-function and a different optimizer called Nadam [1] for faster convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(data):\n",
    "    \"\"\"\n",
    "    Constructs a fully-connected neural network model for the given dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.array[samples, features]\n",
    "        the image dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : keras.Model\n",
    "        the fully-connected neural network\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(20, activation='tanh', kernel_regularizer=l2(0.1), input_shape=(data.shape[1],)))\n",
    "    model.add(Dense(5, activation='tanh', kernel_regularizer=l2(0.1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Nadam(lr=1e-4), loss='mae')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(train_data)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking the Training Progress\n",
    "\n",
    "Tracking the training history of a Keras model using a callback is already familiar to you. Here, we have made a slight modification to only track the loss epoch-wise. In this case, the $\\texttt{history}$ object returned by Keras' $\\texttt{fit}$ call, would provide you with the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory(Callback):\n",
    "    \"\"\"\n",
    "    Class for tracking the training progress/history of the neural network. Implements the keras.Callback interface.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs):\n",
    "        self.loss = []\n",
    "        self.validation_loss = []\n",
    "            \n",
    "    def on_epoch_end(self, _, logs):\n",
    "        \"\"\"\n",
    "        Callback invoked after each training batch.\n",
    "        Should track the training loss and accuracy in the respective members.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        _ : int\n",
    "            unused, int corresponding to the batch number\n",
    "        logs : dict{str -> float}\n",
    "            a dictionary mapping from the observed quantity to the actual valu\n",
    "        \"\"\"\n",
    "        if 'loss' in logs:\n",
    "            self.loss.append(logs['loss'])\n",
    "        if 'val_loss' in logs:\n",
    "            self.validation_loss.append(logs['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Training a regression model work exactly the same like a convolutional neural network in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, test_data, test_labels, epochs=500, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains a fully-connected neural network given training and test data/labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.Model\n",
    "        the fully-connected neural network\n",
    "    train_data : np.array[train samples, features]\n",
    "        the training data\n",
    "    train_labels : np.array[train samples]\n",
    "        the labels, aka. the vector containing the ring count\n",
    "    test_data : np.array[test samples, features]\n",
    "        the test data\n",
    "    test_labels : np.array[test samples]\n",
    "        the labels, aka. the vector containing the ring count\n",
    "    epoch: positive int, optional\n",
    "        the number of epochs for which the neural network is trained, defaults to 100\n",
    "    batch_size: positive int, optional\n",
    "        the size of the training batches, defaults to 32\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    history : TrainingHistory\n",
    "        the tracked training and test history\n",
    "    \"\"\"\n",
    "    history = TrainingHistory()\n",
    "    model.fit(\n",
    "        train_data, train_labels, validation_data=(test_data, test_labels),\n",
    "        epochs=epochs, shuffle=True, callbacks=[history]\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "    \n",
    "history = train_model(model, train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Training Progress\n",
    "\n",
    "Using matplotlib, we are plotting the model's loss during the training phase. It should result in an almost textbook-like smooth decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plots the training (batch-wise) and test (epoch-wise) loss and accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history : TrainingHistory\n",
    "        an instance of TrainingHistory monitoring callback\n",
    "    \"\"\"\n",
    "    figure, axis = plt.subplots(1, 1, figsize=(16, 5))\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    axis.set_xlabel('epoch')\n",
    "    axis.set_ylabel('loss')\n",
    "    \n",
    "    epochs = np.arange(len(history.loss))\n",
    "    axis.plot(epochs, history.loss, color='C0', label='loss')\n",
    "    axis.plot(epochs, history.validation_loss, color='C4', label='validation loss')\n",
    "    axis.set_ylim(bottom=0.0)\n",
    "    \n",
    "    # display a legend\n",
    "    axis.legend(loc=1)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Brief Evaluation of the Model\n",
    "\n",
    "Using Keras' evaluate function we can one again obtain the loss of the model on the test data. You should be seeing an MAE of roughly 1.75, meaning that on average we mispredict the number of rings by that number. So is this prediction useful? That is highly dependent on the particular application domain of the predictor. However, we may see whether the network is better than the mere fluctuation within the data.\n",
    "\n",
    "Note: a further optimization of the network will result in an MAE of roughly 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE of the prediction:', model.evaluate(test_data, test_labels))\n",
    "print('Standard deviation of the labels:', test_labels.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] **Incorporating Nesterov Momentum into Adam**, *Dozat, Timothy*, (2016)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
